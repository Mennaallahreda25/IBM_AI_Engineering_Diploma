{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e08647b5647483481648aa186f0d5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/261482368 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b8c3b9621c41458cc5348d50739b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    \"concrete_data_week3/valid\",\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 23:28:39.730883: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2023-10-13 23:28:39.740719: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394325000 Hz\n",
      "2023-10-13 23:28:39.741648: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55af7a666b10 executing computations on platform Host. Devices:\n",
      "2023-10-13 23:28:39.741709: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2023-10-13 23:28:39.790777: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7fb0fd1f3b10>,\n",
       " <keras.layers.core.Dense at 0x7fb0e0e5b950>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fb174c54a50>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7fb1b01b4f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb17083ab90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb1b035a910>,\n",
       " <keras.layers.core.Activation at 0x7fb1708645d0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7fb1707deed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb16ef754d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb1703ddf90>,\n",
       " <keras.layers.core.Activation at 0x7fb1640d3ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb1703a3e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb16409cf90>,\n",
       " <keras.layers.core.Activation at 0x7fb14c7f7990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb14c78cbd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb170793cd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb14c76e390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb14c69b090>,\n",
       " <keras.layers.merge.Add at 0x7fb14c69bf10>,\n",
       " <keras.layers.core.Activation at 0x7fb14c5e5f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb14c507c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb14c575f10>,\n",
       " <keras.layers.core.Activation at 0x7fb14c55fad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb14c47e090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb14c5e5ad0>,\n",
       " <keras.layers.core.Activation at 0x7fb14c477550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb14c390850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb14c331cd0>,\n",
       " <keras.layers.merge.Add at 0x7fb14c3738d0>,\n",
       " <keras.layers.core.Activation at 0x7fb14c290d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb14c232c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb14c208390>,\n",
       " <keras.layers.core.Activation at 0x7fb14c208890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb14c1a7d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb14c146dd0>,\n",
       " <keras.layers.core.Activation at 0x7fb14c146fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb14c03d7d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb12c7e1ad0>,\n",
       " <keras.layers.merge.Add at 0x7fb12c7e1990>,\n",
       " <keras.layers.core.Activation at 0x7fb12c6fec90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb12c734210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb12c6f7150>,\n",
       " <keras.layers.core.Activation at 0x7fb12c6f7690>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb12c611a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb12c579f10>,\n",
       " <keras.layers.core.Activation at 0x7fb12c579950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb12c529790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb12c428f50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb12c487ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb12c3a11d0>,\n",
       " <keras.layers.merge.Add at 0x7fb12c371bd0>,\n",
       " <keras.layers.core.Activation at 0x7fb12c32c1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb12c2e1950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb12c2a8f90>,\n",
       " <keras.layers.core.Activation at 0x7fb12c1e5090>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb12c1e5a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb12c1611d0>,\n",
       " <keras.layers.core.Activation at 0x7fb12c139750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb12c0d57d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb12c03ded0>,\n",
       " <keras.layers.merge.Add at 0x7fb12c03dd50>,\n",
       " <keras.layers.core.Activation at 0x7fb1247add90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb12474a950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb1247166d0>,\n",
       " <keras.layers.core.Activation at 0x7fb124716ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb124652dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb124624310>,\n",
       " <keras.layers.core.Activation at 0x7fb1246244d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb124540610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb12452bf50>,\n",
       " <keras.layers.merge.Add at 0x7fb12452b850>,\n",
       " <keras.layers.core.Activation at 0x7fb124459cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb1243e6210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb1243bfb90>,\n",
       " <keras.layers.core.Activation at 0x7fb1243576d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb1242fde50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb1242d02d0>,\n",
       " <keras.layers.core.Activation at 0x7fb1242d0410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb12426af10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb1241e8fd0>,\n",
       " <keras.layers.merge.Add at 0x7fb124188c10>,\n",
       " <keras.layers.core.Activation at 0x7fb124107550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb124107650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb124080c50>,\n",
       " <keras.layers.core.Activation at 0x7fb124045f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fefe8a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fef40490>,\n",
       " <keras.layers.core.Activation at 0x7fb0fef40750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0feedb2d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fedf7310>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fee46fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fed148d0>,\n",
       " <keras.layers.merge.Add at 0x7fb0fed925d0>,\n",
       " <keras.layers.core.Activation at 0x7fb0fecfaf50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fec1bf10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fec743d0>,\n",
       " <keras.layers.core.Activation at 0x7fb0fec74790>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0febaae50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0feb32250>,\n",
       " <keras.layers.core.Activation at 0x7fb0feb0c610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0feaa6e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fea0ab50>,\n",
       " <keras.layers.merge.Add at 0x7fb0fea0af50>,\n",
       " <keras.layers.core.Activation at 0x7fb0fe9bc8d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fe8da6d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fe934550>,\n",
       " <keras.layers.core.Activation at 0x7fb0fe934e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fe856610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fe83cf50>,\n",
       " <keras.layers.core.Activation at 0x7fb0fe83c850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fe76cd10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fe6fb210>,\n",
       " <keras.layers.merge.Add at 0x7fb0fe6d27d0>,\n",
       " <keras.layers.core.Activation at 0x7fb0fe856b10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fe66add0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fe5e2990>,\n",
       " <keras.layers.core.Activation at 0x7fb0fe5e2190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fe528d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fe4fa5d0>,\n",
       " <keras.layers.core.Activation at 0x7fb0fe4faf90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fe414810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fe37ee50>,\n",
       " <keras.layers.merge.Add at 0x7fb0fe37ed10>,\n",
       " <keras.layers.core.Activation at 0x7fb0fe32dd50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fe5282d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fe295f50>,\n",
       " <keras.layers.core.Activation at 0x7fb0fe295750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fe1c5690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fe1a7c50>,\n",
       " <keras.layers.core.Activation at 0x7fb0fe1a7590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fe0d9a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fe03f650>,\n",
       " <keras.layers.merge.Add at 0x7fb0fe03fa10>,\n",
       " <keras.layers.core.Activation at 0x7fb0fdfd6850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fdffbe90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fdf54c90>,\n",
       " <keras.layers.core.Activation at 0x7fb0fdf54e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fdeece90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fde66150>,\n",
       " <keras.layers.core.Activation at 0x7fb0fde66210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fdd82050>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fdd6ac10>,\n",
       " <keras.layers.merge.Add at 0x7fb0fdcdcc50>,\n",
       " <keras.layers.core.Activation at 0x7fb0fdc9fe90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fdd824d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fdc01110>,\n",
       " <keras.layers.core.Activation at 0x7fb0fdc01410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fdbb8690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fdb16890>,\n",
       " <keras.layers.core.Activation at 0x7fb0fdb16dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fdab37d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fd96cd90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fda2c110>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fd901050>,\n",
       " <keras.layers.merge.Add at 0x7fb0fd849250>,\n",
       " <keras.layers.core.Activation at 0x7fb0fd860f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fd7a71d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fd7c6c90>,\n",
       " <keras.layers.core.Activation at 0x7fb0fd7e1e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fd77b450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fd6f1f10>,\n",
       " <keras.layers.core.Activation at 0x7fb0fd68d210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fd612c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fd59c110>,\n",
       " <keras.layers.merge.Add at 0x7fb0fd5ad390>,\n",
       " <keras.layers.core.Activation at 0x7fb0fd50add0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fd50ab10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fd4a1050>,\n",
       " <keras.layers.core.Activation at 0x7fb0fd467f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fd425b10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fd3a1350>,\n",
       " <keras.layers.core.Activation at 0x7fb0fd3a1750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fb0fd33d4d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fb0fd2b6f50>,\n",
       " <keras.layers.merge.Add at 0x7fb0fd251250>,\n",
       " <keras.layers.core.Activation at 0x7fb0fd1d7cd0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7fb0fd50a410>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7fb16ef75950>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      " 11/301 [>.............................] - ETA: 4:39:41 - loss: 0.4473 - acc: 0.7982"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
